KerasでVGG16
2016/12/27

今回は、Deep Learningの画像応用において代表的なモデルであるVGG16をKerasから使ってみた。この学習済みのVGG16モデルは画像に関するいろいろな面白い実験をする際の基礎になるためKerasで取り扱う方法をちゃんと理解しておきたい。

# VGG16の概要

VGG16は畳み込み層13（間にMax-Pooling含む）とフル結合層3の計16層から成る畳み込みニューラルネットワーク。層の数が多いだけで一般的な畳み込みニューラルネットと大きな違いはないわかりやすいモデル。同時期に出たGoogLeNetに比べるとシンプルでわかりやすい。実装も簡単。

ImageNetと呼ばれる大規模な画像データを使って訓練したVGG16が一般公開されている。

- ImageNetのデータ構造
- 大学などの研究機関にしか配布されていない
- クローリングする必要があるが今回は学習済みモデルを使うので不要


# KerasのVGG16モデル

KerasではVGG16モデルが`keras.applications.vgg16`モジュールに実装されているため簡単に使える。少し古いバージョンのKerasだと自分でモデル構造を書いて、.h5ファイル形式の重みをダウンロードする必要があった（参考: [VGG16 model for Keras](https://gist.github.com/baraldilorenzo/07d7802847aaad0a35d3)）が最新の1.2.0では不要になっている。バックエンドに合わせて変換された重みファイルを自動ダウンロードしてくれる。

```python
from keras.applications.vgg16 import VGG16
model = VGG16(include_top=True, weights='imagenet', input_tensor=None, input_shape=None)
```

`VGG16`クラスは4つの引数を取る。

- `include_top`はVGG16のトップにある1000クラス分類するフル結合層（FC）を含むか含まないかを指定する。今回は画像分類を行いたいためFCを含んだ状態で使う。FCを捨ててVGG16を特徴抽出器として使うことでいろいろ面白いことができるがまた今度取り上げたい。
- `weights`はVGG16の重みの種類を指定する。VGG16は単にモデル構造であるため必ずしもImageNetを使って学習しなければいけないわけではない。しかし、現状ではImageNetで学習した重みしか提供されていない。`None`にするとランダム重みになる。自分で集めた画像で学習する猛者はこちらか？
- `input_tensor`は自分でモデルに画像を入力したいときに使うが今回は未使用。あとでVGG16のFine-tuningをする際に使う。
- `input_shape`は入力画像の形状を指定する。`include_top=True`にして画像分類器として使う場合は `(224, 224, 3)` で固定なので`None`でOK。何か中途半端な解像度だけどこれがImageNetの標準サイズのようだ。

読み込んだモデルをちょっと調べてみよう。

```python
% print(model)
<keras.engine.training.Model at 0x2b05220aa978>
```

どうやら`VGG16`はKerasで一般的な`Sequential`モデルではなく、別のクラスのようだ。`dir(model)`をするとわかるが、`Sequential`モデルで層を積み重ねるのによく使っていた`add()`がないので注意。たとえば、VGG16に新たに層を付け加えるときに[ちょっとした工夫がいる](https://github.com/fchollet/keras/issues/4040)。これもあとで詳しく取り上げたい。

`summary()`するとモデル構造が見える。

```python
% model.summary()
____________________________________________________________________________________________________
Layer (type)                     Output Shape          Param #     Connected to
====================================================================================================
input_26 (InputLayer)            (None, 224, 224, 3)   0
____________________________________________________________________________________________________
block1_conv1 (Convolution2D)     (None, 224, 224, 64)  1792        input_26[0][0]
____________________________________________________________________________________________________
block1_conv2 (Convolution2D)     (None, 224, 224, 64)  36928       block1_conv1[0][0]
____________________________________________________________________________________________________
block1_pool (MaxPooling2D)       (None, 112, 112, 64)  0           block1_conv2[0][0]
____________________________________________________________________________________________________
block2_conv1 (Convolution2D)     (None, 112, 112, 128) 73856       block1_pool[0][0]
____________________________________________________________________________________________________
block2_conv2 (Convolution2D)     (None, 112, 112, 128) 147584      block2_conv1[0][0]
____________________________________________________________________________________________________
block2_pool (MaxPooling2D)       (None, 56, 56, 128)   0           block2_conv2[0][0]
____________________________________________________________________________________________________
block3_conv1 (Convolution2D)     (None, 56, 56, 256)   295168      block2_pool[0][0]
____________________________________________________________________________________________________
block3_conv2 (Convolution2D)     (None, 56, 56, 256)   590080      block3_conv1[0][0]
____________________________________________________________________________________________________
block3_conv3 (Convolution2D)     (None, 56, 56, 256)   590080      block3_conv2[0][0]
____________________________________________________________________________________________________
block3_pool (MaxPooling2D)       (None, 28, 28, 256)   0           block3_conv3[0][0]
____________________________________________________________________________________________________
block4_conv1 (Convolution2D)     (None, 28, 28, 512)   1180160     block3_pool[0][0]
____________________________________________________________________________________________________
block4_conv2 (Convolution2D)     (None, 28, 28, 512)   2359808     block4_conv1[0][0]
____________________________________________________________________________________________________
block4_conv3 (Convolution2D)     (None, 28, 28, 512)   2359808     block4_conv2[0][0]
____________________________________________________________________________________________________
block4_pool (MaxPooling2D)       (None, 14, 14, 512)   0           block4_conv3[0][0]
____________________________________________________________________________________________________
block5_conv1 (Convolution2D)     (None, 14, 14, 512)   2359808     block4_pool[0][0]
____________________________________________________________________________________________________
block5_conv2 (Convolution2D)     (None, 14, 14, 512)   2359808     block5_conv1[0][0]
____________________________________________________________________________________________________
block5_conv3 (Convolution2D)     (None, 14, 14, 512)   2359808     block5_conv2[0][0]
____________________________________________________________________________________________________
block5_pool (MaxPooling2D)       (None, 7, 7, 512)     0           block5_conv3[0][0]
____________________________________________________________________________________________________
flatten (Flatten)                (None, 25088)         0           block5_pool[0][0]
____________________________________________________________________________________________________
fc1 (Dense)                      (None, 4096)          102764544   flatten[0][0]
____________________________________________________________________________________________________
fc2 (Dense)                      (None, 4096)          16781312    fc1[0][0]
____________________________________________________________________________________________________
predictions (Dense)              (None, 1000)          4097000     fc2[0][0]
====================================================================================================
Total params: 138,357,544
Trainable params: 138,357,544
Non-trainable params: 0
____________________________________________________________________________________________________
```

今回は、`include_top=True`なので`fc1`、`fc2`、`predictions`という層が追加されているのが確認できる。また、最後の`predictions`層の形状が `(None, 1000)` で1000クラスの分類であることもわかる。`None`はサイズが決まっていないことを意味し、ここでは入力サンプル数（入力バッチ数）を意味する。

# VGG16で一般物体認識

VGG16モデルが読み込めたのでさっそく画像を入力して分類するプログラムを書いてみよう。今回はコマンドラインから分類したい画像ファイル名を引数として入力するようにした。実際は、VGG16のロードに時間がかかるので起動後にプロンプトでファイル名を入力できるようにした方がよさそう。

```python
from keras.applications.vgg16 import VGG16, preprocess_input, decode_predictions
from keras.preprocessing import image
import numpy as np
import sys

if len(sys.argv) != 2:
    print("usage: python test_vgg16.py [image file]")
    sys.exit(1)

filename = sys.argv[1]

# 学習済みのVGG16をロード
# 構造とともに学習済みの重みも読み込まれる
model = VGG16(weights='imagenet')
# model.summary()

# 引数で指定した画像ファイルを読み込む
# サイズはVGG16のデフォルトである224x224にリサイズされる
img = image.load_img(filename, target_size=(224, 224))

# 読み込んだPIL形式の画像をarrayに変換
x = image.img_to_array(img)

# 3次元テンソル（rows, cols, channels) を
# 4次元テンソル (samples, rows, cols, channels) に変換
x = np.expand_dims(x, axis=0)

# Top-5のクラスを予測する
# VGG16の1000クラスはdecode_predictions()で文字列に変換される
preds = model.predict(preprocess_input(x))
results = decode_predictions(preds, top=5)[0]
for result in results:
    print(result)

```

今回は、複数の画像をまとめて入力せずに1枚だけ入力するようにした（実際はバッチ単位で1000枚入力してまとめて予測も可）。

画像の入力は`keras.preprocessing.image`モジュールを使うといろいろ便利。`load_img()`で指定したサイズにリサイズして画像がロードできる。また、`img_to_array()`でPIL形式の画像をNumPy array形式に変換できる。

`load_img()`でロードした画像は `(rows, cols, channels)` の3Dテンソルなのでこれにサンプル数 `samples` を追加した4Dテンソルに変換する必要がある。

クラスの予測は`predict()`で行う。VGG16用の平均を引く前処理 `preprocess_input()` を通した4Dテンソルを入力とする。`predict()`の戻り値はNNの出力であり1000クラスの確率値である。このままではどのクラスが何なのか非常にわかりづらい。VGG16用の`decode_predictions()`を使うと確率値が高い順にクラス名を出力してくれる。

いくつか適当な画像を入力して認識結果を見てみよう。

- ImageNetのデータで1000クラスに含まれる場合
- 犬・猫は種類まで判定できる
- 花は難しい（ImageNetにないかそもそもクラスがない）


# 参考
